---
title: "oliver > martin"
author: "Oliver"
date: "4/28/2019"
output: html_document
---

```{r}
library(mosaic)
library(DescTools)
library(lmtest)
```

```{r}
fullComments <- read.csv("fullComments.csv")
```

```{r}
kitchenSink <- glm(Controversiality ~ WordCount + Gilded + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = fullComments, family = "binomial")

msummary(kitchenSink)

#LRT full model vs. intercept only 
lrtest(kitchenSink)
```

```{r}
#kitchen sink model just without the gilded variable
without_gilded <- glm(Controversiality ~ WordCount + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = fullComments, family = "binomial")

msummary(without_gilded)

#LRT full model vs w/o gilded
lrtest(kitchenSink, without_gilded)
```

# so we choose the model without gilded and compare that to model without moderator
```{r}
#w/o gilded or moderator
without_moderator <- glm(Controversiality ~ WordCount + WKND + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = fullComments, family = "binomial")

#LRT full model vs w/o gilded
lrtest(without_gilded, without_moderator)
```

```{r}
final_model <- without_gilded

msummary(final_model)
car::vif(final_model)
```

## randomization test for moderator
```{r}
# original (observed) slope for comparison
observed_slope_Moderator <- coef(final_model)["ModeratorTRUE"]

# for reproducibility, use set.seed()
set.seed(2)

# change the number within set.seed if you want different randomly shuffled values
slopetest <- do(100) * (glm(Controversiality ~ WordCount + WKND + shuffle(Moderator) + 
    comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + relative_sentiment_diff, family = "binomial", 
    data = fullComments))
```

## Create Plot for moderator
```{r}
# create a density plot to compare the distribution of slopes get from 
gf_density(~ModeratorTRUE, data=slopetest, xlab="Slope Coefficients for Shuffled Moderator") %>%
  gf_vline(xintercept = ~ observed_slope_Moderator, color="red")

favstats(~ModeratorTRUE, data=slopetest)
quantile(~ModeratorTRUE, data=slopetest, p=c(0.01,0.025,0.05))

```

## randomization test for comments_in_subreddit       
```{r}
# original (observed) slope for comparison
observed_slope_Comments <- coef(final_model)["comments_in_subreddit"]

# for reproducibility, use set.seed()
set.seed(5)

# change the number within set.seed if you want different randomly shuffled values
slopetest2 <- do(100) * (glm(Controversiality ~ WordCount + WKND + Moderator + 
    shuffle(comments_in_subreddit) + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + relative_sentiment_diff, family = "binomial", 
    data = fullComments))
```

## Create Plot for comments_in_subreddit       
```{r}
# create a density plot to compare the distribution of slopes get from 
gf_density(~comments_in_subreddit, data=slopetest2, xlab="Slope Coefficients for Shuffled comments_in_subreddit") %>%
  gf_vline(xintercept = ~ observed_slope_Comments, color="red")

favstats(~comments_in_subreddit, data=slopetest2)
quantile(~comments_in_subreddit, data=slopetest2, p=c(0.01,0.025,0.05))

```

## randomization test for relative_sentiment_diff
```{r}
# original (observed) slope for comparison
observed_slope_diff <- coef(final_model)["relative_sentiment_diff"]

# for reproducibility, use set.seed()
set.seed(5)

# change the number within set.seed if you want different randomly shuffled values
slopetest3 <- do(100) * (glm(Controversiality ~ WordCount + WKND + Moderator + 
    comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + shuffle(relative_sentiment_diff), family = "binomial", 
    data = fullComments))
```

## Create Plot for relative_sentiment_diff       
```{r}
# create a density plot to compare the distribution of slopes get from 
gf_density(~relative_sentiment_diff, data=slopetest3, xlab="Slope Coefficients for Shuffled comments_in_subreddit") %>%
  gf_vline(xintercept = ~ observed_slope_diff, color="red")

favstats(~relative_sentiment_diff, data=slopetest3)
quantile(~relative_sentiment_diff, data=slopetest3, p=c(0.01,0.025,0.05))

```

## test different models
### split into training and testing
```{r}
# randomly select half of the observations to be in training dataset
randomnums <- fullComments %>%
  mutate(randomnum = rnorm(nrow(fullComments))) %>%
  arrange(randomnum) 

train.set <- randomnums[1:198999,]

# save other half of the observations to be in test dataset
test.set <- randomnums[199000:397998,]
```

### find model
```{r}
# full model
train_model1 <- glm(Controversiality ~ WordCount + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

msummary(train_model1)

# get rid of moderator
train_no_moderator <- glm(Controversiality ~ WordCount + WKND + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

# low pvalue so keep moderator
lrtest(train_no_moderator, train_model1)

# get rid of comments
train_no_comments <- glm(Controversiality ~ WordCount + WKND + Moderator + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

# high pvalue so get rid of comments
lrtest(train_model1, train_no_comments)

# full model minus comments
train_model2 <- glm(Controversiality ~ WordCount + WKND + Moderator + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

# full model (minus comments) without moderator
train_no_moderator2 <- glm(Controversiality ~ WordCount + WKND + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

# low pvalue so still keep moderator
lrtest(train_no_moderator2, train_model2)

final_train_model <- train_no_moderator2
msummary(final_train_model)
```

### MAKE FUNCTION
```{r}
my.expit <- function(x) {
  y <- exp(x) / (1+exp(x))
  return(y)
}

accuracy <- function(model) {
  
  predictions <- test.set %>% 
  mutate(predicted.logodds = predict(model, newdata = test.set),
         
         predicted.prob2 = my.expit(predicted.logodds),
         
         # change the cutoff HERE
         classified.outcome = ifelse(predicted.prob2 > 0.1, yes=1, no=0))

  acc <- (tally(Controversiality ~ classified.outcome, data = predictions)[1,1] + 
            tally(Controversiality ~ classified.outcome, data = predictions)[2,2]) /
      sum(tally(Controversiality ~ classified.outcome, data = predictions))
  
  return(acc)
}

```

#Martin addition
```{r}
set.seed(123)
filtered <- fullComments %>% 
  filter(WordCount > 10 | Score > 5 | Score < -5) %>% 
  filter(Controversiality==1)

4481/198832
randomnums <- filtered %>%
  mutate(randomnum = rnorm(nrow(filtered))) %>%
  arrange(randomnum) 

#train.set <- randomnums[1:101669,]
#train.set <- randomnums[1:196212,]
train.set <- randomnums[1:99416,]

# save other half of the observations to be in test dataset
#test.set <- randomnums[101670:203338,]
#test.set <- randomnums[196213:392425,]
test.set <- randomnums[99417:198832,]



martin_model<- glm(Controversiality ~ WordCount +  SentimentQDAP + SentimentGI+ subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM , data = train.set, family = "binomial")


msummary(martin_model)
accuracy(martin_model)

without_gilded1 <- glm(Controversiality ~ WordCount + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data =  filtered, family = "binomial")

accuracy(without_gilded1)

simple_model <- glm(Controversiality ~ WordCount +  subreddit_scaled_total_mean , data =  filtered, family = "binomial")

accuracy(simple_model)

model1<- glm(Controversiality ~ SentimentQDAP + Moderator, data =filtered, family = "binomial")

accuracy(model1)
```

