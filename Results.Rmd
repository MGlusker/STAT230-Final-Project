---
title: "Results"
author: "Oliver"
date: "5/6/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mosaic)
library(DescTools)
library(lmtest)
library(SentimentAnalysis)
library(dplyr)
library(corrplot)
library(leaps)
library(tidyverse)
library(knitr)
```

# Results

Typically, results sections start with descriptive statistics, e.g. what percent of the sample is male/female, what is the mean GPA overall, in the different groups, etc. Figures can be nice to illustrate these differences! However, information presented must be relevant in helping to answer the research question(s) of interest. Typically, inferential (i.e. hypothesis tests) statistics come next. Tables can often be helpful for results from multiple regression. Do not give computer output here! This should look like a peer-reviewed journal article results section. Tables and figures should be labeled, embedded in the text, and referenced appropriately. The results section typically makes for fairly dry reading. It does not explain the impact of findings, it merely highlights and reports statistical information. 

## Data Frame

The researchers' initial data frame was obtained from http://files.pushshift.io/reddit/comments/daily/ and contained all of the comments from the entirity Reddit for one day. They collected four days worth of data (two weekends and two weekdays) and took a random sample of 100,000 comments from each day to end up with a total of roughly 400,000 comments across four days of Reddit content. With this data frame the researchers then added columns that used the R package SentimentAnalysis to keep track of the sentiment of each comment according to different sentiment dictionaries. They also created metrics that kept track of the average sentiment of each comments Subreddit as well as the difference between a comment's sentiment and the average sentiment of their Subreddit. The final dataframe then contained 397,998 observations with 18 variables. The variable of interest that the researchers hoped to predict was controversiality, a binary variable where 1 indicates a comment is controversial, and 0 indicates a comment is not controversial. In the entire dataset, 2.28% of comments were controversial (9,064 controversial comments).

```{r, echo=FALSE}
fullComments <- read.csv("fullComments.csv")

tally(~Controversiality, data=fullComments)
```

The researchers then decided to split the dataframe into a training set and testing set so that they could determine a model based on half of the data and then test the accuracy of that model on the second half of the data. 

```{r, echo=FALSE, include=FALSE}
#SPLIT DATA INTO TRAIN AND TEST
set.seed(2)
# randomly select half of the observations to be in training dataset
randomnums <- fullComments %>%
  mutate(randomnum = rnorm(nrow(fullComments))) %>%
  arrange(randomnum) 

train.set <- randomnums[1:198999,]

# save other half of the observations to be in test dataset
test.set <- randomnums[199000:397998,]
```

## Initial Modeling 

For variable selection, the researchers decided to start with a model that used all of the newly created variables (as well as all of the original variables from the inital dataframe) and use likelihood ratio tests to determine which variables were the best at predicting controversiality. The initial model showed that only the variables *Gilded* and *Moderator* were not significant at the 0.05 level. A series of likelihood ratio tests were thus preformed to determine whether or not these variables should be included in our final model. The researchers determined that the variables *Gilded* and *Moderator* both were not significant predictors and did not add anything to the model. From the first series of models, the best model that was found thus included 7 predictor variables and can be seen below in *Table 1.1*. 

```{r, include=FALSE}
fullModel <- glm(Controversiality ~ WordCount + Gilded + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

msummary(fullModel)

#LRT full model vs. intercept only 
lrtest(fullModel)

#full model without the gilded variable
without_gilded <- glm(Controversiality ~ WordCount + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

msummary(without_gilded)

#LRT full model vs w/o gilded
lrtest(fullModel, without_gilded)
# so we determine that we don't want Gilded

#w/o gilded or moderator
without_moderator_gilded <- glm(Controversiality ~ WordCount + WKND + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = train.set, family = "binomial")

#LRT full model vs w/o gilded
lrtest(without_gilded, without_moderator_gilded)
# so we determine that we also don't want Moderator

#so final model is the full model minus Gilded and Moderator
final_model <- without_moderator_gilded
```

```{r, echo=FALSE}
msummary(final_model)
```
*Table 1.1*

## Randomization Tests

In the final model only three variables had p-values that were greater than 0.001: *comments_in_subreddit*, *WKND*, and *relative_sentiment_diff* (with p-values of 0.0336, 0.0051, and 0.0035 respectively). To confirm that these variables were significant, the researchers decided to perform a randomization test on each to determine if the observed coefficients were statistically significant or not. The results of these randomization tests can be seen in Figures *2.1*, *2.2*, and *2.3* below where the red line indicates the observed coefficient and the density plot shows the randomization distribution. If the red line falls far outside of the density plot, this means that the observed coefficient would not be expected to found by chance and is thus statistically significant. While the researchers were only able to run 100 randomization tests (as opposed to the 10,000 that they hoped to) for each variable, this was due to the limit on computing power that they had access to. This explains the imperfect normal distribution of the plots below, but as the observed coefficients for each predictor fell outside of each randomization distribution, the researchers were able to conclude that each of the three predictors were indeed statstically significant, as the initial p-values for each suggested.

### Figure 2.1
```{r, echo=FALSE}
#RANDOMIZATION TEST FOR COMMENTS_IN_SUBREDDIT
# original (observed) slope for comparison
observed_slope_Comments <- coef(final_model)["comments_in_subreddit"]

# for reproducibility, use set.seed()
set.seed(2)

# change the number within set.seed if you want different randomly shuffled values
slopetest2 <- do(100) * (glm(Controversiality ~ WordCount + WKND + 
    shuffle(comments_in_subreddit) + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + relative_sentiment_diff, family = "binomial", 
    data = train.set))

# create a density plot to compare the distribution of slopes get from 
gf_density(~comments_in_subreddit, data=slopetest2, xlab="Slope Coefficients for Shuffled comments_in_subreddit") %>%
  gf_vline(xintercept = ~ observed_slope_Comments, color="red")
```

### Figure 2.2
```{r, echo=FALSE}
#RANDOMIZATION TEST FOR WKND
# original (observed) slope for comparison
observed_slope_WKND <- coef(final_model)["WKNDTRUE"]

# for reproducibility, use set.seed()
set.seed(2)

# change the number within set.seed if you want different randomly shuffled values
slopetest2 <- do(100) * (glm(Controversiality ~ WordCount + shuffle(WKND) + 
    comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + relative_sentiment_diff, family = "binomial", 
    data = train.set))

# create a density plot to compare the distribution of slopes get from 
gf_density(~WKNDTRUE, data=slopetest2, xlab="Slope Coefficients for Shuffled WKND") %>%
  gf_vline(xintercept = ~ observed_slope_WKND, color="red")
```

### Figure 2.3
```{r, echo=FALSE}
#RANDOMIZATION TEST FOR RELATIVE_SENTIMENT_DIFF
# original (observed) slope for comparison
observed_slope_diff <- coef(final_model)["relative_sentiment_diff"]

# for reproducibility, use set.seed()
set.seed(2)

# change the number within set.seed if you want different randomly shuffled values
slopetest3 <- do(100) * (glm(Controversiality ~ WordCount + WKND + 
    comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + 
    subreddit_diff_LM + shuffle(relative_sentiment_diff), family = "binomial", 
    data = train.set))

# create a density plot to compare the distribution of slopes get from 
gf_density(~relative_sentiment_diff, data=slopetest3, xlab="Slope Coefficients for Shuffled relative_sentiment_diff") %>%
  gf_vline(xintercept = ~ observed_slope_diff, color="red")
```

## Testing Accuracy of Model

With a final model in hand with statistically significant predictors, the researchers then created a function to predict the accuracy of that model using the testing set that was created at the beginning. The researcher's found that the accuracy of the final model was 96.3%, but that the accuracy of the intercept only model was 97.7%.

```{r, echo=FALSE}
my.expit <- function(x) {
  y <- exp(x) / (1+exp(x))
  return(y)
}

accuracy <- function(model) {
  
  predictions <- test.set %>% 
  mutate(predicted.logodds = predict(model, newdata = test.set),
         
         predicted.prob2 = my.expit(predicted.logodds),
         
         # change the cutoff HERE
         classified.outcome = ifelse(predicted.prob2 > 0.5, yes=1, no=0))

  acc <- (tally(Controversiality ~ classified.outcome, data = predictions)[1,1] + 
            tally(Controversiality ~ classified.outcome, data = predictions)[2,2]) /
      sum(tally(Controversiality ~ classified.outcome, data = predictions))
  
  return(acc)
}

accuracy(final_model)

intercept_only_accuracy <- 388934 / (388934 + 9064)
intercept_only_accuracy

```

## Next Steps: Balanced Dataset

Reserachers were concerned with the efficacy of a model that predicted a variable with so few observations. The researchers then decided to resample the initial dataframe in order to have more observations where a comment was marked as controversial (as compared with the 2.28% of comments). This new, balanced dataset, contained exactly 50% of comments that were controversial and 50% of comments that were not controversial. The researchers again split the data into training and testing subsets.  

```{r, echo=FALSE}
#filtering the data 
controversial <- fullComments %>% 
  filter(Controversiality == 1)

noncontroversial <- fullComments %>% 
  filter(Controversiality != 1) 

n <- sample(noncontroversial$X, size = 9064, replace = FALSE)

sampled_noncontroversial <- noncontroversial %>% 
  filter(noncontroversial$X  %in%  n)

balanced_data <- rbind(controversial, sampled_noncontroversial)

tally(~Controversiality, data=balanced_data)
```

```{r, include=FALSE}
#SPLIT DATA INTO TRAIN AND TEST
set.seed(55)
# randomly select half of the observations to be in training dataset
randomnums <- balanced_data %>%
  mutate(randomnum = rnorm(nrow(balanced_data))) %>%
  arrange(randomnum) 

balanced_train.set <- randomnums[1:9063,]

# save other half of the observations to be in test dataset
balanced_test.set <- randomnums[9064:18128,]
```

## Modeling with Balanced Dataset

The researhers used the same process with the balanced dataset as with the original dataset to determine the best model to predict controversiality. The final model that was determined by the researchers contained 5 predictor variables and the can be seen below in *Table 3.1*. It's important to note that the *WKND* and *comments_in_subreddit* variables were found to be non-statistically significant using htis balanced dataset (while they were found to be statistically significant in the researcher's final model using the original dataset). 

```{r, include=FALSE}
balanced_fullModel <- glm(Controversiality ~ WordCount + WKND + Moderator + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = balanced_train.set, family = "binomial")

msummary(balanced_fullModel)

#LRT full model vs. intercept only 
lrtest(fullModel)

#full model without the moderator variable
balanced_without_moderator <- glm(Controversiality ~ WordCount + WKND + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = balanced_train.set, family = "binomial")

#LRT full model vs w/o gilded
lrtest(balanced_fullModel, balanced_without_moderator)
# so we determine that we don't want moderator

#w/o moderator or WKND
balanced_without_moderator_WKND <- glm(Controversiality ~ WordCount + comments_in_subreddit + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = balanced_train.set, family = "binomial")

#LRT 
lrtest(balanced_without_moderator, balanced_without_moderator_WKND)
# so we determine that we also don't want WKND

# w/o moderator, WKND, or comments
balanced_without_moderator_WKND_comments <- glm(Controversiality ~ WordCount + subreddit_scaled_total_mean + subreddit_diff_QDAP + subreddit_diff_LM +  relative_sentiment_diff, data = balanced_train.set, family = "binomial")

#LRT
lrtest(balanced_without_moderator_WKND, balanced_without_moderator_WKND_comments)
# so we determine that we also don't want comments

#so final model is the full model minus Gilded, Moderator, WKND, and comments_in_subreddit
balanced_final_model <- balanced_without_moderator_WKND_comments
lrtest(balanced_final_model)
```

```{r, echo=FALSE}
msummary(balanced_final_model)
```

The researchers then found the accuracy of this model to be 56.4%, as compared with the 50% accuracy of the intercept only model. 

```{r, echo=FALSE}
my.expit <- function(x) {
  y <- exp(x) / (1+exp(x))
  return(y)
}

balanced_accuracy <- function(model) {
  
  predictions <- balanced_test.set %>% 
  mutate(predicted.logodds = predict(model, newdata = balanced_test.set),
         
         predicted.prob2 = my.expit(predicted.logodds),
         
         # change the cutoff HERE
         classified.outcome = ifelse(predicted.prob2 > 0.5, yes=1, no=0))

  acc <- (tally(Controversiality ~ classified.outcome, data = predictions)[1,1] + 
            tally(Controversiality ~ classified.outcome, data = predictions)[2,2]) /
      sum(tally(Controversiality ~ classified.outcome, data = predictions))
  
  return(acc)
}

balanced_accuracy(balanced_final_model)

intercept_only_accuracy <- 9064 / (9064 + 9064)
intercept_only_accuracy
```
