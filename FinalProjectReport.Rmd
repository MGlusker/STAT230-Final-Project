---
title: "STAT230--Final Project"
author: "Noah Soloman, Oliver Baldwin Edwards, and Martin Glusker"
output:
  pdf_document: default
---
##Abstract
The goal of this study was to determine which online comments on the social media website Reddit.com gain attention, defined as whether a comment was controversial, based on sentiment analysis and other variables. Multiple logistic regression was used to predict controversiality with sentiment analysis variables and other variables on ~400,000 comments worth of data in addition to a smaller, balanced dataset with a one-to-one match of controversial comments to non-controversial comments. No remarkable results were found using the original dataset as none of our models differed meaningfully from the intercept only model, but using the balanced dataset the best model had an accuracy of 57.430%, relative the intercept only model accuracy of 50%. The best model was *Controversiality* ~ *WordCount* + *subreddit_scaled_total_mean* + *subreddit_diff_QDAP* + *subreddit_diff_LM* + *relative_sentiment_diff*. Sentiment predictors were found to be associated with controversy, and significantly so due to the large data set, but this does not make the sentiment predictors meaningful as there remains a lot of randomness not contained in these sentiment predictors. 

##Background and Significance
This project examined how people interact on online forums and social media, an area of particularly relevance in an age increasingly defined by how people interact with one another online. Given the anonymous nature of many internet forums, there has been much speculation about the toxicity of these forums, asking in particular whether these forums incentivize mean of derisive comments? Does the average sentiment of a particular community (in the case of this study, a subreddit) influence which comments are rewarded and popular? For example, do toxic communities reward toxic comments? Or is there another relationship at play?

The primary goal of this study was to see what sort of comments garner attention in the online forums Reddit.com. Researchers were interested in examining the relationship between the sentiment of a comment, and that comment’s controversiality (a binary output produced by Reddit). Researchers also looked at the whether the overall average sentiment of a subreddit (a sub-community within Reddit) effects whether positive or negative comments on that subreddit garner attention. For example researchers hypothesized that a subreddit such as ‘r/aww’, dedicated to sharing photos of cute animals and which was expected to have a very positive average sentiment, would reward comments with positive sentiment much more than negative sentiment. This trend was also hypothesized to apply to toxic subreddits on Reddit, and the relationship between negative and positive comments and controversiality in the context of more negative communities.

##Methods
###Data collection 
These data were collected via a census of comments posted on reddit.com on 01/01/18, 
14/01/18, 01/02/18, 17/02/18 yielding approximately 8 million comments. For usability, 100,000 of the comments from each day were then selected at random and the rest were discarded. The data set was then cleaned by removing duplicate rows and those with formatting errors, resulting in around 399,000 samples in the final data set. After modeling with this initial dataset, the researchers determined that the percentage of non-controversial comments was too small, as only ~2% of comments in the dataset were controversial. A balanced dataset was created, where all controversial comments were included, and a equal number of randomly selected non-controversial comments were selected. 


###Variable creation 
Response variable examined was score (equal to upvotes-downvotes), predicted using Stefan Feuerriegel & Nicolas Proellochs’ Sentiment Analysis package using the QDAP dictionary compiled by Tyler Rinker. In particular, sentiment analysis was run on the body of the comment, and SentimentQDAP, NegativityQDAP, and PositivityQDAP were used as predictor variables. NegativityQDAP is a quantitative value of how negative the comment was, based on the particular dictionary used, in this case QDAP. In a similar vein, PositivityQDAP is a vlaue of how positive the value is. SentimentQDAP is simply positivity - negativity.

Two new categories of variables were also created, one associated with the comment's sentiment relative to its subreddit's average sentiment. This variable, called 'subreddit_scaled_total_mean' represents the mean of all four different sentiment variables for each subreddit, which were then scaled, and the mean taken of the four scaled mean sentiment variables for each subreddit. This represents an average sentiment score (in units of standard deviation, as it’s scaled) for each subreddit. Difference variables were then created, which represent the difference between a specific comment’s sentiment and its subreddit’s average sentiment. This was done in nominal terms for each of the four sentiment libraries, in addition to a scaled version that represents all four variables. 


###Analytic Methods
Multiple logistic regression was used to study the association between controversy and SentimentQDAP, NegativityQDAP, PositivityQDAP, and Moderator as well as related factors. The researchers conducted drop in deviance tests to examine which sets of variables produce an effective model. Accuracy on a testing subset of the data was also used as a metric for determining model quality. Accuracy was defined as (correct predictions/total sample size). Finally, randomization tests were then used to increase the robustness of the conclusions.


##Results
Typically, results sections start with descriptive statistics, e.g. what percent of the sample is male/female, what is the mean GPA overall, in the different groups, etc. Figures can be nice to illustrate these differences! However, information presented must be relevant in helping to answer the research question(s) of interest. Typically, inferential (i.e. hypothesis tests) statistics come next. Tables can often be helpful for results from multiple regression. Do not give computer output here! This should look like a peer-reviewed journal article results section. Tables and figures should be labeled, embedded in the text, and referenced appropriately. The results section typically makes for fairly dry reading. It does not explain the impact of findings, it merely highlights and reports statistical information. 

##Conclusion
The objective for this study was to determine whether the sentiment of a comment was a factor in determining how much attention a comment drew. In addition, the researchers looked at other non-sentiment variables to see whether they also proved to be good at predicting how much attention a comment received. The binary variable of controversiality was used the parameter for “attention a comment receives.”

We find that the sentiment of a comment and other variables associated with each comment included in our model prove to only marginally better than the intercept only model in our sampled dataset, and very slightly worse than the intercept only model in our unsampled dataset. 

In the sampled dataset, where the researchers sampled the non-controversial comments so that there was a one-to-one match of non-controversial comments to controversial comments, our model did somewhat better than the intercept only model. No one model stood out as much better than the others, with the best models having an accuracy between 57.613-57.792%. The top model, as determined by accuracy, included the following variables as predictors: WordCount, WKND, Moderator, comments_in_subreddit,  SentimentQDAP, SentimentGI, SentimentLM, SentimentHE, subreddit_scaled_total_mean, subreddit_diff_QDAP, subreddit_diff_LM, relative_sentiment_diff. The researchers attempted to simplify this model, but even the simplest top model included 7 variables. A simpler model with just 4 predictors was found, with variables WordCount, SentimentQDAP, subreddit_scaled_total_mean, and  subreddit_diff_QDAP  as predictors and with an average accuracy of 56.852%. The simplest model the researchers found, while remaining a good model, used subreddit_scaled_total_mean as its lone predictor variable and had an accuracy of 56.545%.
Ultimately, this means that a lot of factors aid in predicting whether a comment is controversial or not, and even then, the models only add 6-7% of accuracy relative to the intercept only model, and that is only in the reduced dataset. This means that it is difficult to predict controversiality based on the variables in the dataset. In the full data set, our models perform even worse, with the models performing 0.07% worse than the intercept only. In conclusion, we determine that sentiment and related variables, at least in our specific dataset, add some, but not much information as predictors.

**Restate your objective and draw connections between your analyses and objective. In other words, how did (or didnt) you answer/address your objective. Place these all in the larger scope of previous research on your topic (i.e. what you found from the literature review), that is, how do your findings help the field move forward? Talk about the limitations of your findings and possible areas for future research to better investigate your research question. End with a concluding sentence or two that summarizes your key findings and impact on the field. Note this is your last opportunity to clarify the scope of your findings before a journalist misinterprets them and makes wild extrapolations! Protect yourself by being clear about what is not implied by your research.**

##Limitations & Areas for Concern
The scope of this study is limited by its design in a number of ways. First of all, this is an observation study, not a randomized experiment, thus association between the variables considered can be analyzed, but causal claims would not be appropriate. These limitations can be grouped into three broad categories; computation time, dataset characteristics, and complications from data cleaning. 

One of the primary problems encountered in this research was the lack of computation time available to the researchers. The initial data set included close to 8 million observations of some two dozen columns and running operations on a set of this magnitude rendered unfeasible run times on the hardware used. A more full analysis would have ideally incorporated a greater number of days of comments, but this was simply not feasible given the computational constraints. As such, caution must be taken in broadly applying any of the conclusions of this study outside of the date range analyzed. 

Second, there are fundamental limit in the substance of the data set that limit the conclusions that can be drawn. The analysis is limited to Reddit.com and is not necessarily representative of internet comments more generally as Reddit.com has a user base that tends to lean younger and more male than other internet forums. In addition, the nature of Reddit is such that a handful of very popular communities (subreddits) tend to produce the vast majority of the comments, meaning that comments from these popular subreddits tended to dominate the data set, limiting the conclusions of this study to the larger subreddits. In addition, of the four days selected, at least one of them was in close temporal proximity to a major world event, vis. the shooting in Parkland Florida. As such it is possible that comments collected from days that were in close proximity to high level world events may not be fully representative of Reddit comments in general. Another potential limiting factor is the presence of ‘bot’ comments in the data sets. ‘Bots’ are Reddit accounts that follow automated protocols producing generally formulaic content for moderation or similar purposes. The researchers were not able to remove these bot comments from the data set and their presence means that the claims made in this study cannot be unconditionally applied to human comment only contexts. This likely does not confound the data at all, but does make associations more challenging to detect. Lastly there are limitations for this study derived from the data cleaning process. Duplicate comments were only able to be removed after the sample was pared down to ~400,000 comments due to computational time limits, meaning comments that were appeared more than once in the original ~8 million observation data set were disproportionately likely to make it into the final data set. That being said, the relative infrequency of duplicate comments and the size of the data set render this very unlikely to meaningfully influence the conclusions. 

Lastly, a variety of malformations were present in the original uncleaned data arising from, among other things, unusual or improperly escaped characters in the body of the comment. As such, the incidence of these malformations was greater among longer comments, so the removal of malformed rows increased the proportion of short comments to long comments. Only about 400,000 comments were removed in this fashion, or about 5% of the original data set, so the proportion is not likely to have meaningfully shifted.

##Areas for Further Research
    As mentioned above, one of the primary limitations of this study was the limited computing power the researchers had access to. As such, an easy first step for further research would be to repeat the experiment with a computation cluster that allowed for larger scale data analysis. In addition to using a larger subset of each day’s worth of comments, a more full analysis would involve sampling a larger number of days so as to reduce variation from world events and increase the scope of the study. A second key area for expansion and improvement in this study would be to use a purpose built sentiment analysis dictionary designed for analyzing internet comments. Doing so would likely increase the capability of researchers to draw meaningful conclusions about the relationship between sentiment and controversiality. Finally, to more fully expand the scope of this study, data could be tabulated from sites other than Reddit in order to capture a more full picture of the controversy-sentiment relation on internet forums.
    
    
###References 
Data collected by Reddit User u/glitch_in_the_matrix, hosted on pushshift.io by Jason Baumgartner (http://files.pushshift.io/Reddit/comments/daily/)

###Appendix The appendix can include any additional data exploration and modeling done in the process that was not included in the ”final” models presented in the main report.


